{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvWIItAe-0fN"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/crunchdao/quickstarters/blob/master/competitions/structural-break/quickstarters/baseline/baseline.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNUXnJa_-0fO"
      },
      "source": [
        "![Banner](https://raw.githubusercontent.com/crunchdao/quickstarters/refs/heads/master/competitions/structural-break/assets/banner.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lurIF1Ve-0fP"
      },
      "source": [
        "# ADIA Lab Structural Break Challenge\n",
        "\n",
        "## Challenge Overview\n",
        "\n",
        "Welcome to the ADIA Lab Structural Break Challenge! In this challenge, you will analyze univariate time series data to determine whether a structural break has occurred at a specified boundary point.\n",
        "\n",
        "### What is a Structural Break?\n",
        "\n",
        "A structural break occurs when the process governing the data generation changes at a certain point in time. These changes can be subtle or dramatic, and detecting them accurately is crucial across various domains such as climatology, industrial monitoring, finance, and healthcare.\n",
        "\n",
        "![Structural Break Example](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/competitions/structural-break/quickstarters/baseline/images/example.png)\n",
        "\n",
        "### Your Task\n",
        "\n",
        "For each time series in the test set, you need to predict a score between `0` and `1`:\n",
        "- Values closer to `0` indicate no structural break at the specified boundary point;\n",
        "- Values closer to `1` indicate a structural break did occur.\n",
        "\n",
        "### Evaluation Metric\n",
        "\n",
        "The evaluation metric is [ROC AUC (Area Under the Receiver Operating Characteristic Curve)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html), which measures the performance of detection algorithms regardless of their specific calibration.\n",
        "\n",
        "- ROC AUC around `0.5`: No better than random chance;\n",
        "- ROC AUC approaching `1.0`: Perfect detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF3ghfYVK3t2"
      },
      "source": [
        "# Setup\n",
        "\n",
        "The first steps to get started are:\n",
        "1. Get the setup command\n",
        "2. Execute it in the cell below\n",
        "\n",
        "### >> https://hub.crunchdao.com/competitions/structural-break/submit/notebook\n",
        "\n",
        "![Reveal token](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/documentation/animations/reveal-token.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DUeixiC_IJM",
        "outputId": "ab3e5194-4a23-4ff2-d448-45427c401319"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "crunch-cli, version 6.5.0\n",
            "main.py: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/submissions/21403/main.py (49073 bytes)\n",
            "notebook.ipynb: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/submissions/21403/notebook.ipynb (102634 bytes)\n",
            "requirements.txt: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/submissions/21403/requirements.original.txt (206 bytes)\n",
            "data/X_train.parquet: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
            "data/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
            "data/y_train.parquet: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
            "data/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
            "                                \n",
            "---\n",
            "Success! Your environment has been correctly setup.\n",
            "Next recommended actions:\n",
            "1. Load the Crunch Toolings: `crunch = crunch.load_notebook()`\n",
            "2. Execute the cells with your code\n",
            "3. Run a test: `crunch.test()`\n",
            "4. Download and submit your code to the platform!\n"
          ]
        }
      ],
      "source": [
        "# # Install the Crunch CLI\n",
        "# %pip install --upgrade crunch-cli\n",
        "\n",
        "# # Setup your local environment\n",
        "# !crunch setup --notebook structural-break hello --token aaaabbbbccccddddeeeeffff\n",
        "%pip install crunch-cli --upgrade --quiet --progress-bar off\n",
        "!crunch setup-notebook structural-break 49Uj7NG28SSPE9LovUPCKGGh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if True:\n",
        "    !pip install xgboost\n",
        "    !pip install lightgbm\n",
        "    !pip install torch\n",
        "    !pip install rich\n",
        "    !pip install nolds"
      ],
      "metadata": {
        "id": "GTqNg8z3MMV1",
        "outputId": "f32a9c0a-01db-4235-f393-18f067bb91a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.15.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
            "Collecting nolds\n",
            "  Downloading nolds-0.6.1-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>1.0 in /usr/local/lib/python3.11/dist-packages (from nolds) (2.0.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from nolds) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nolds) (75.2.0)\n",
            "Downloading nolds-0.6.1-py2.py3-none-any.whl (225 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nolds\n",
            "Successfully installed nolds-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IBhw7hv-0fQ"
      },
      "source": [
        "# Your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpLeMWSw-0fQ"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-18T09:52:21.302334Z",
          "start_time": "2024-11-18T09:52:18.268241Z"
        },
        "id": "MKqz-6Zw-0fR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import typing\n",
        "\n",
        "# Import your dependencies\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import sklearn.metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from typing import List, Tuple, Dict, Union, Optional\n",
        "from pydantic import BaseModel, Field, validator\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import entropy\n",
        "from scipy.special import kl_div\n",
        "# Import your dependencies\n",
        "import os\n",
        "import typing\n",
        "\n",
        "\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import sklearn.metrics\n",
        "\n",
        "import numpy as np\n",
        "from typing import Tuple, Literal, List, Dict, Any, Optional, Union\n",
        "from pydantic import BaseModel, Field, field_validator\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.nn.functional import binary_cross_entropy_with_logits\n",
        "import statsmodels.api as sm\n",
        "import nolds\n",
        "from rich import print\n",
        "from scipy import signal\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "# import crunch"
      ],
      "metadata": {
        "id": "jRyP85QnL9nK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries\n",
        "class TestResult(BaseModel):\n",
        "    \"\"\"Base class for test results\"\"\"\n",
        "    test_name: str\n",
        "    statistic: float\n",
        "    p_value: float\n",
        "    reject_null: bool\n",
        "    alpha: float = 0.05\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return (f\"{self.test_name} Test Result:\\n\"\n",
        "                f\"  Statistic: {self.statistic:.6f}\\n\"\n",
        "                f\"  p-value: {self.p_value:.6f}\\n\"\n",
        "                f\"  Reject Null Hypothesis: {self.reject_null} (α = {self.alpha})\")\n",
        "\n",
        "class KSTestResult(TestResult):\n",
        "    \"\"\"Kolmogorov-Smirnov test results\"\"\"\n",
        "    test_name: str = \"Kolmogorov-Smirnov\"\n",
        "\n",
        "class ADTestResult(TestResult):\n",
        "    \"\"\"Anderson-Darling test results\"\"\"\n",
        "    test_name: str = \"Anderson-Darling\"\n",
        "    critical_values: List[float] = Field(default_factory=list)\n",
        "    significance_levels: List[float] = Field(default_factory=list)\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        base_str = super().__str__()\n",
        "        crit_vals = \"\\n  \".join([f\"{sl:.3f}: {cv:.3f}\" for sl, cv in\n",
        "                                zip(self.significance_levels, self.critical_values)])\n",
        "        return (f\"{base_str}\\n\"\n",
        "                f\"  Critical Values:\\n  {crit_vals}\")\n",
        "\n",
        "class MannWhitneyTestResult(TestResult):\n",
        "    \"\"\"Mann-Whitney U test results\"\"\"\n",
        "    test_name: str = \"Mann-Whitney U\"\n",
        "\n",
        "class KLDivergenceResult(BaseModel):\n",
        "    \"\"\"Kullback-Leibler divergence results\"\"\"\n",
        "    test_name: str = \"Kullback-Leibler Divergence\"\n",
        "    kl_xy: float  # KL(x||y)\n",
        "    kl_yx: float  # KL(y||x)\n",
        "    symmetric_kl: float  # Symmetric KL\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return (f\"{self.test_name} Result:\\n\"\n",
        "                f\"  KL(x||y): {self.kl_xy:.6f}\\n\"\n",
        "                f\"  KL(y||x): {self.kl_yx:.6f}\\n\"\n",
        "                f\"  Symmetric KL: {self.symmetric_kl:.6f}\\n\"\n",
        "                f\"  Note: Higher values indicate more dissimilar distributions\")\n",
        "\n",
        "class PermutationTestResult(TestResult):\n",
        "    \"\"\"Permutation test results\"\"\"\n",
        "    test_name: str = \"Permutation\"\n",
        "    n_permutations: int\n",
        "    test_statistic_name: str\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        base_str = super().__str__()\n",
        "        return (f\"{base_str}\\n\"\n",
        "                f\"  Test Statistic: {self.test_statistic_name}\\n\"\n",
        "                f\"  Number of Permutations: {self.n_permutations}\")\n",
        "\n",
        "class TimeSeriesComparison:\n",
        "    \"\"\"\n",
        "    A class to compare two time series using various statistical tests to determine\n",
        "    if they are drawn from the same or different distributions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, default_alpha: float = 0.05):\n",
        "        \"\"\"\n",
        "        Initialize the TimeSeriesComparison class.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        default_alpha : float, optional\n",
        "            Default significance level for hypothesis tests (default is 0.05)\n",
        "        \"\"\"\n",
        "        self.default_alpha = default_alpha\n",
        "\n",
        "    def _validate_inputs(self, x: Union[List, np.ndarray], y: Union[List, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Validate input time series data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        x, y : array-like\n",
        "            Time series data\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        Tuple[np.ndarray, np.ndarray]\n",
        "            Validated numpy arrays\n",
        "        \"\"\"\n",
        "        x_array = np.asarray(x, dtype=float)\n",
        "        y_array = np.asarray(y, dtype=float)\n",
        "\n",
        "        # Check for NaN values\n",
        "        if np.isnan(x_array).any() or np.isnan(y_array).any():\n",
        "            raise ValueError(\"Input data contains NaN values\")\n",
        "\n",
        "        # Ensure we have enough data points\n",
        "        if len(x_array) < 2 or len(y_array) < 2:\n",
        "            raise ValueError(\"Both time series must have at least 2 data points\")\n",
        "\n",
        "        return x_array, y_array\n",
        "\n",
        "    def kolmogorov_smirnov_test(self, x: Union[List, np.ndarray], y: Union[List, np.ndarray],\n",
        "                               alpha: Optional[float] = None) -> KSTestResult:\n",
        "        \"\"\"\n",
        "        Perform Kolmogorov-Smirnov test on two time series.\n",
        "\n",
        "        Null Hypothesis (H0): The two samples come from the same continuous distribution.\n",
        "        Alternative Hypothesis (H1): The two samples come from different distributions.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        x, y : array-like\n",
        "            Time series data\n",
        "        alpha : float, optional\n",
        "            Significance level (default is self.default_alpha)\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        KSTestResult\n",
        "            Results of the KS test\n",
        "        \"\"\"\n",
        "        alpha = alpha if alpha is not None else self.default_alpha\n",
        "        x_array, y_array = self._validate_inputs(x, y)\n",
        "\n",
        "        # Perform KS test\n",
        "        statistic, p_value = stats.ks_2samp(x_array, y_array)\n",
        "\n",
        "        # Determine if we reject the null hypothesis\n",
        "        reject_null = p_value < alpha\n",
        "\n",
        "        return KSTestResult(\n",
        "            statistic=statistic,\n",
        "            p_value=p_value,\n",
        "            reject_null=reject_null,\n",
        "            alpha=alpha\n",
        "        )\n",
        "\n",
        "    def anderson_darling_test(self, x: Union[List, np.ndarray], y: Union[List, np.ndarray],\n",
        "                             alpha: Optional[float] = None) -> ADTestResult:\n",
        "        \"\"\"\n",
        "        Perform Anderson-Darling test on two time series.\n",
        "\n",
        "        Null Hypothesis (H0): The two samples come from the same continuous distribution.\n",
        "        Alternative Hypothesis (H1): The two samples come from different distributions.\n",
        "\n",
        "        Note: This implements the k-sample Anderson-Darling test, which tests whether samples\n",
        "        come from the same distribution without specifying what that distribution is.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        x, y : array-like\n",
        "            Time series data\n",
        "        alpha : float, optional\n",
        "            Significance level (default is self.default_alpha)\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        ADTestResult\n",
        "            Results of the Anderson-Darling test\n",
        "        \"\"\"\n",
        "        alpha = alpha if alpha is not None else self.default_alpha\n",
        "        x_array, y_array = self._validate_inputs(x, y)\n",
        "\n",
        "        # Combine samples and create sample indices\n",
        "        samples = [x_array, y_array]\n",
        "\n",
        "        # Perform Anderson-Darling test\n",
        "        result = stats.anderson_ksamp(samples)\n",
        "\n",
        "        # Extract results\n",
        "        statistic = result.statistic\n",
        "        p_value = result.significance_level\n",
        "        significance_levels = [0.25, 0.1, 0.05, 0.025, 0.01]\n",
        "        critical_values = result.critical_values\n",
        "\n",
        "        # Determine if we reject the null hypothesis\n",
        "        reject_null = p_value < alpha\n",
        "\n",
        "        return ADTestResult(\n",
        "            statistic=statistic,\n",
        "            p_value=p_value,\n",
        "            reject_null=reject_null,\n",
        "            alpha=alpha,\n",
        "            critical_values=list(critical_values),\n",
        "            significance_levels=significance_levels\n",
        "        )\n",
        "\n",
        "    def mann_whitney_test(self, x: Union[List, np.ndarray], y: Union[List, np.ndarray],\n",
        "                        alpha: Optional[float] = None, alternative: str = 'two-sided') -> MannWhitneyTestResult:\n",
        "        \"\"\"\n",
        "        Perform Mann-Whitney U test on two time series.\n",
        "\n",
        "        Null Hypothesis (H0): The distributions of both samples are equal.\n",
        "        Alternative Hypothesis (H1): The distributions of the two samples are not equal\n",
        "                                    (or one has a larger median than the other, depending\n",
        "                                    on the 'alternative' parameter).\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        x, y : array-like\n",
        "            Time series data\n",
        "        alpha : float, optional\n",
        "            Significance level (default is self.default_alpha)\n",
        "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
        "            Defines the alternative hypothesis:\n",
        "            - 'two-sided': distributions are different\n",
        "            - 'less': the distribution of x is stochastically less than the distribution of y\n",
        "            - 'greater': the distribution of x is stochastically greater than the distribution of y\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        MannWhitneyTestResult\n",
        "            Results of the Mann-Whitney U test\n",
        "        \"\"\"\n",
        "        alpha = alpha if alpha is not None else self.default_alpha\n",
        "        x_array, y_array = self._validate_inputs(x, y)\n",
        "\n",
        "        # Perform Mann-Whitney U test\n",
        "        statistic, p_value = stats.mannwhitneyu(x_array, y_array, alternative=alternative)\n",
        "\n",
        "        # Determine if we reject the null hypothesis\n",
        "        reject_null = p_value < alpha\n",
        "\n",
        "        return MannWhitneyTestResult(\n",
        "            statistic=statistic,\n",
        "            p_value=p_value,\n",
        "            reject_null=reject_null,\n",
        "            alpha=alpha\n",
        "        )\n",
        "\n",
        "    def kl_divergence(self, x: Union[List, np.ndarray], y: Union[List, np.ndarray],\n",
        "                     bins: int = 20, smoothing: float = 1e-10) -> KLDivergenceResult:\n",
        "        \"\"\"\n",
        "        Calculate Kullback-Leibler divergence between two time series.\n",
        "\n",
        "        Note: KL divergence is not a statistical test but a measure of how one probability\n",
        "        distribution diverges from another. It is not symmetric, meaning KL(x||y) ≠ KL(y||x).\n",
        "        Higher values indicate distributions that are more different from each other.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        x, y : array-like\n",
        "            Time series data\n",
        "        bins : int, optional\n",
        "            Number of bins to use in histogram (default is 20)\n",
        "        smoothing : float, optional\n",
        "            Small value added to probability estimates to avoid zeros (default is 1e-10)\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        KLDivergenceResult\n",
        "            Results of the KL divergence calculation\n",
        "        \"\"\"\n",
        "        x_array, y_array = self._validate_inputs(x, y)\n",
        "\n",
        "        # Get the range for the histograms - use the same range for both to ensure comparable results\n",
        "        min_val = min(np.min(x_array), np.min(y_array))\n",
        "        max_val = max(np.max(x_array), np.max(y_array))\n",
        "\n",
        "        # Create histograms\n",
        "        hist_x, bin_edges = np.histogram(x_array, bins=bins, range=(min_val, max_val), density=True)\n",
        "        hist_y, _ = np.histogram(y_array, bins=bins, range=(min_val, max_val), density=True)\n",
        "\n",
        "        # Add smoothing to avoid zeros\n",
        "        hist_x = hist_x + smoothing\n",
        "        hist_y = hist_y + smoothing\n",
        "\n",
        "        # Normalize to ensure proper probability distributions\n",
        "        hist_x = hist_x / np.sum(hist_x)\n",
        "        hist_y = hist_y / np.sum(hist_y)\n",
        "\n",
        "        # Calculate KL divergence in both directions\n",
        "        kl_xy = entropy(hist_x, hist_y)  # KL(x||y)\n",
        "        kl_yx = entropy(hist_y, hist_x)  # KL(y||x)\n",
        "\n",
        "        # Symmetric KL divergence (average of both directions)\n",
        "        symmetric_kl = (kl_xy + kl_yx) / 2\n",
        "\n",
        "        return KLDivergenceResult(\n",
        "            kl_xy=kl_xy,\n",
        "            kl_yx=kl_yx,\n",
        "            symmetric_kl=symmetric_kl\n",
        "        )\n",
        "\n",
        "    def permutation_test(self, x: Union[List, np.ndarray], y: Union[List, np.ndarray],\n",
        "                        n_permutations: int = 1000, alpha: Optional[float] = None,\n",
        "                        statistic: str = 'mean') -> PermutationTestResult:\n",
        "        \"\"\"\n",
        "        Perform a permutation test on two time series.\n",
        "\n",
        "        Null Hypothesis (H0): The two samples come from the same distribution.\n",
        "        Alternative Hypothesis (H1): The two samples come from different distributions.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        x, y : array-like\n",
        "            Time series data\n",
        "        n_permutations : int, optional\n",
        "            Number of permutations to perform (default is 1000)\n",
        "        alpha : float, optional\n",
        "            Significance level (default is self.default_alpha)\n",
        "        statistic : str, optional\n",
        "            Statistic to use for the test ('mean', 'median', 'variance', 'std', or 'ks')\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        PermutationTestResult\n",
        "            Results of the permutation test\n",
        "        \"\"\"\n",
        "        alpha = alpha if alpha is not None else self.default_alpha\n",
        "        x_array, y_array = self._validate_inputs(x, y)\n",
        "\n",
        "        # Define the test statistic function\n",
        "        if statistic == 'mean':\n",
        "            def calc_statistic(a, b):\n",
        "                return np.abs(np.mean(a) - np.mean(b))\n",
        "        elif statistic == 'median':\n",
        "            def calc_statistic(a, b):\n",
        "                return np.abs(np.median(a) - np.median(b))\n",
        "        elif statistic == 'variance':\n",
        "            def calc_statistic(a, b):\n",
        "                return np.abs(np.var(a) - np.var(b))\n",
        "        elif statistic == 'std':\n",
        "            def calc_statistic(a, b):\n",
        "                return np.abs(np.std(a) - np.std(b))\n",
        "        elif statistic == 'ks':\n",
        "            def calc_statistic(a, b):\n",
        "                return stats.ks_2samp(a, b)[0]  # Return the KS statistic\n",
        "        else:\n",
        "            raise ValueError(\"Invalid statistic. Choose from 'mean', 'median', 'variance', 'std', or 'ks'\")\n",
        "\n",
        "        # Calculate observed test statistic\n",
        "        observed_statistic = calc_statistic(x_array, y_array)\n",
        "\n",
        "        # Combine data for permutation\n",
        "        combined = np.concatenate([x_array, y_array])\n",
        "        n_x = len(x_array)\n",
        "        n_y = len(y_array)\n",
        "        n_total = n_x + n_y\n",
        "\n",
        "        # Perform permutation test\n",
        "        count = 0\n",
        "        for _ in range(n_permutations):\n",
        "            # Shuffle the combined data\n",
        "            np.random.shuffle(combined)\n",
        "\n",
        "            # Split into two groups of original sizes\n",
        "            perm_x = combined[:n_x]\n",
        "            perm_y = combined[n_x:]\n",
        "\n",
        "            # Calculate test statistic for permuted data\n",
        "            perm_statistic = calc_statistic(perm_x, perm_y)\n",
        "\n",
        "            # Count how many permutations have a test statistic >= observed\n",
        "            if perm_statistic >= observed_statistic:\n",
        "                count += 1\n",
        "\n",
        "        # Calculate p-value\n",
        "        p_value = count / n_permutations\n",
        "\n",
        "        # Determine if we reject the null hypothesis\n",
        "        reject_null = p_value < alpha\n",
        "\n",
        "        return PermutationTestResult(\n",
        "            statistic=observed_statistic,\n",
        "            p_value=p_value,\n",
        "            reject_null=reject_null,\n",
        "            alpha=alpha,\n",
        "            n_permutations=n_permutations,\n",
        "            test_statistic_name=statistic\n",
        "        )\n",
        "\n",
        "    def run_all_tests(self, x: Union[List, np.ndarray], y: Union[List, np.ndarray],\n",
        "                     alpha: Optional[float] = None, permutation_statistic: str = 'mean',\n",
        "                     n_permutations: int = 1000, kl_bins: int = 20) -> Dict:\n",
        "        \"\"\"\n",
        "        Run all available tests on two time series.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        x, y : array-like\n",
        "            Time series data\n",
        "        alpha : float, optional\n",
        "            Significance level (default is self.default_alpha)\n",
        "        permutation_statistic : str, optional\n",
        "            Statistic to use for the permutation test (default is 'mean')\n",
        "        n_permutations : int, optional\n",
        "            Number of permutations for the permutation test (default is 1000)\n",
        "        kl_bins : int, optional\n",
        "            Number of bins for KL divergence calculation (default is 20)\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        Dict\n",
        "            Dictionary containing results of all tests\n",
        "        \"\"\"\n",
        "        alpha = alpha if alpha is not None else self.default_alpha\n",
        "\n",
        "        results = {\n",
        "            'ks_test': self.kolmogorov_smirnov_test(x, y, alpha),\n",
        "            'ad_test': self.anderson_darling_test(x, y, alpha),\n",
        "            'mw_test': self.mann_whitney_test(x, y, alpha),\n",
        "            'kl_divergence': self.kl_divergence(x, y, bins=kl_bins),\n",
        "            'permutation_test': self.permutation_test(x, y, n_permutations=n_permutations,\n",
        "                                                    alpha=alpha, statistic=permutation_statistic)\n",
        "        }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def plot_distributions(self, x: Union[List, np.ndarray], y: Union[List, np.ndarray],\n",
        "                         bins: int = 20, figsize: Tuple[int, int] = (10, 6)) -> plt.Figure:\n",
        "        \"\"\"\n",
        "        Plot histograms and kernel density estimates of the two time series.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        x, y : array-like\n",
        "            Time series data\n",
        "        bins : int, optional\n",
        "            Number of bins for histograms (default is 20)\n",
        "        figsize : tuple, optional\n",
        "            Figure size (default is (10, 6))\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        plt.Figure\n",
        "            Matplotlib figure containing the plots\n",
        "        \"\"\"\n",
        "        x_array, y_array = self._validate_inputs(x, y)\n",
        "\n",
        "        # Create figure with two subplots\n",
        "        fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
        "\n",
        "        # Plot histograms\n",
        "        axes[0].hist(x_array, bins=bins, alpha=0.5, label='Series X')\n",
        "        axes[0].hist(y_array, bins=bins, alpha=0.5, label='Series Y')\n",
        "        axes[0].set_title('Histogram Comparison')\n",
        "        axes[0].set_xlabel('Value')\n",
        "        axes[0].set_ylabel('Frequency')\n",
        "        axes[0].legend()\n",
        "\n",
        "        # Plot kernel density estimates\n",
        "        from scipy.stats import gaussian_kde\n",
        "        x_density = gaussian_kde(x_array)\n",
        "        y_density = gaussian_kde(y_array)\n",
        "\n",
        "        # Create a common x axis for plotting\n",
        "        min_val = min(np.min(x_array), np.min(y_array))\n",
        "        max_val = max(np.max(x_array), np.max(y_array))\n",
        "        xs = np.linspace(min_val, max_val, 1000)\n",
        "\n",
        "        axes[1].plot(xs, x_density(xs), label='Series X')\n",
        "        axes[1].plot(xs, y_density(xs), label='Series Y')\n",
        "        axes[1].set_title('Kernel Density Estimate')\n",
        "        axes[1].set_xlabel('Value')\n",
        "        axes[1].set_ylabel('Density')\n",
        "        axes[1].legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "class SyntheticDataGenerator(BaseModel):\n",
        "    n_series: int\n",
        "    pct_true: float  # Between 0 and 1\n",
        "    min_length: int = 50\n",
        "    max_length: int = 200\n",
        "    seed: int = 42\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    def _random_distribution(self, name: str, size: int, params: dict) -> np.ndarray:\n",
        "        dist_map = {\n",
        "            'normal': lambda: np.random.normal(params.get('loc', 0), params.get('scale', 1), size),\n",
        "            't': lambda: stats.t.rvs(df=params.get('df', 5), size=size),\n",
        "            'exponential': lambda: np.random.exponential(params.get('scale', 1), size),\n",
        "            'binomial': lambda: np.random.binomial(n=params.get('n', 10), p=params.get('p', 0.5), size=size),\n",
        "        }\n",
        "        if name not in dist_map:\n",
        "            raise ValueError(f\"Unsupported distribution: {name}\")\n",
        "        return dist_map[name]()\n",
        "\n",
        "    def _generate_series(self, id_val: int, has_break: bool) -> Tuple[pd.DataFrame, bool]:\n",
        "        np.random.seed(self.seed + id_val)\n",
        "\n",
        "        length = np.random.randint(self.min_length, self.max_length + 1)\n",
        "        breakpoint = np.random.randint(length // 3, length - 10) if has_break else np.random.randint(length // 2, length)\n",
        "\n",
        "        # Randomly pick a base distribution\n",
        "        dist_name = np.random.choice(['normal', 't', 'exponential', 'binomial'])\n",
        "        base_params = {\n",
        "            'normal': {'loc': 0, 'scale': 1},\n",
        "            't': {'df': 5},\n",
        "            'exponential': {'scale': 1},\n",
        "            'binomial': {'n': 10, 'p': 0.5}\n",
        "        }[dist_name]\n",
        "\n",
        "        # Generate values\n",
        "        pre_values = self._random_distribution(dist_name, breakpoint, base_params)\n",
        "\n",
        "        if has_break:\n",
        "            # Change distribution parameters\n",
        "            changed_params = {\n",
        "                'normal': {'loc': 1, 'scale': 1.5},\n",
        "                't': {'df': 2},\n",
        "                'exponential': {'scale': 2},\n",
        "                'binomial': {'n': 10, 'p': 0.8}\n",
        "            }[dist_name]\n",
        "            post_values = self._random_distribution(dist_name, length - breakpoint, changed_params)\n",
        "        else:\n",
        "            post_values = self._random_distribution(dist_name, length - breakpoint, base_params)\n",
        "\n",
        "        values = np.concatenate([pre_values, post_values])\n",
        "        period = np.array([0]*breakpoint + [1]*(length - breakpoint))\n",
        "\n",
        "        df = pd.DataFrame({\n",
        "            'value': values,\n",
        "            'period': period\n",
        "        }, index=pd.MultiIndex.from_product([[id_val], range(length)], names=['id', 'time']))\n",
        "\n",
        "        return df, has_break\n",
        "\n",
        "    def generate(self) -> Tuple[pd.DataFrame, pd.Series]:\n",
        "        true_count = int(self.pct_true * self.n_series)\n",
        "        false_count = self.n_series - true_count\n",
        "        labels = [True]*true_count + [False]*false_count\n",
        "        np.random.shuffle(labels)\n",
        "\n",
        "        series_list = []\n",
        "        y_dict = {}\n",
        "\n",
        "        for id_val, has_break in enumerate(labels):\n",
        "            df, label = self._generate_series(id_val, has_break)\n",
        "            series_list.append(df)\n",
        "            y_dict[id_val] = label\n",
        "\n",
        "        X = pd.concat(series_list)\n",
        "        y = pd.Series(y_dict, name='structural_breakpoint')\n",
        "\n",
        "        return X, y\n",
        "\n",
        "class ETLPipeline(BaseModel):\n",
        "    X: pd.DataFrame = pd.DataFrame(dtype='float64')\n",
        "    # y: pd.DataFrame = pd.DataFrame(dtype='float64')\n",
        "    y: pd.Series = pd.Series(dtype='float64')\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True  # Allow non-pydantic types like pd.DataFrame\n",
        "\n",
        "    @field_validator('X')\n",
        "    def validate_X(cls, v):\n",
        "        if not isinstance(v.index, pd.MultiIndex):\n",
        "            raise ValueError(\"X must have a MultiIndex of ['id', 'time']\")\n",
        "        if 'value' not in v.columns or 'period' not in v.columns:\n",
        "            raise ValueError(\"X must contain 'value' and 'period' columns\")\n",
        "        return v\n",
        "\n",
        "    @field_validator('y')\n",
        "    def validate_y(cls, v):\n",
        "        if not isinstance(v, pd.Series):\n",
        "            raise ValueError(\"y must be a pandas Series\")\n",
        "        if v.dtype != 'bool':\n",
        "            raise ValueError(\"y must be of dtype 'bool'\")\n",
        "        return v\n",
        "\n",
        "    def get_ids(self) -> list:\n",
        "        \"\"\"Returns a list of all unique ids in the X set.\"\"\"\n",
        "        return list(self.X.index.get_level_values('id').unique())\n",
        "\n",
        "    def get_series_by_id(self, id_val: int) -> pd.DataFrame:\n",
        "        \"\"\"Returns the time series data for a specific id.\"\"\"\n",
        "        if (id_val not in self.y.index) & len(self.y)>0:\n",
        "            raise ValueError(f\"id {id_val} not found in y\")\n",
        "        try:\n",
        "            return self.X.loc[id_val]\n",
        "        except KeyError:\n",
        "            raise ValueError(f\"id {id_val} not found in X\")\n",
        "\n",
        "    def get_target_by_id(self, id_val: int) -> bool:\n",
        "        \"\"\"Returns the target value for a specific id.\"\"\"\n",
        "        return self.y.loc[id_val]\n",
        "\n",
        "    def get_structural_breakdown(self) -> pd.Series:\n",
        "        \"\"\"Returns the proportion of True/False in y\"\"\"\n",
        "        return self.y.value_counts(normalize=True).rename(\"proportion\")\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(1)\n",
        "\n",
        "class MLModelPipeline(BaseModel):\n",
        "    X: pd.DataFrame\n",
        "    y: pd.Series\n",
        "    X_train: Optional[pd.DataFrame] = None\n",
        "    y_train: Optional[pd.Series] = None\n",
        "    X_test: Optional[pd.DataFrame] = None\n",
        "    y_test: Optional[pd.Series] = None\n",
        "    model_name: str = Field(default=\"logistic_regression\")\n",
        "    model: Optional[Any] = None\n",
        "    test_size: float = 0.2\n",
        "    random_state: int = 42\n",
        "    device: str = Field(default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    @field_validator('X')\n",
        "    def validate_X(cls, v):\n",
        "        if not isinstance(v, pd.DataFrame):\n",
        "            raise ValueError(\"X must be a pandas DataFrame\")\n",
        "        return v\n",
        "\n",
        "    @field_validator('y')\n",
        "    def validate_y(cls, v):\n",
        "        if not isinstance(v, pd.Series):\n",
        "            raise ValueError(\"y must be a pandas Series\")\n",
        "        if v.dtype != bool:\n",
        "            raise ValueError(\"y must be a boolean Series\")\n",
        "        return v\n",
        "\n",
        "    def _initialize_model(self, input_dim: int = None):\n",
        "        if self.model_name == \"logistic_regression\":\n",
        "            return LogisticRegression(max_iter=1000)\n",
        "        elif self.model_name == \"random_forest\":\n",
        "            return RandomForestClassifier(n_estimators=100)\n",
        "        elif self.model_name == \"xgboost\":\n",
        "            return xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "        elif self.model_name == \"lightgbm\":\n",
        "            return lgb.LGBMClassifier()\n",
        "        elif self.model_name == \"mlp\":\n",
        "            if input_dim is None:\n",
        "                raise ValueError(\"input_dim required for MLP model\")\n",
        "            return MLP(input_dim).to(self.device)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model: {self.model_name}\")\n",
        "\n",
        "    def fit_v0(self):\n",
        "        # Align feature index with label index\n",
        "        X_aligned = self.X.loc[self.y.index.intersection(self.X.index)]\n",
        "        y_aligned = self.y.loc[X_aligned.index]\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_aligned, y_aligned, test_size=self.test_size, random_state=self.random_state\n",
        "        )\n",
        "\n",
        "        self.X_train, self.X_test = X_train, X_test\n",
        "        self.y_train, self.y_test = y_train, y_test\n",
        "\n",
        "        if self.model_name == \"mlp\":\n",
        "            input_dim = X_train.shape[1]\n",
        "            self.model = self._initialize_model(input_dim)\n",
        "            self._train_mlp(X_train, y_train)\n",
        "        else:\n",
        "            self.model = self._initialize_model()\n",
        "            self.model.fit(X_train, y_train)\n",
        "\n",
        "    def _train_mlp_v0(self, X_train: pd.DataFrame, y_train: pd.Series):\n",
        "        X_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(self.device)\n",
        "        y_tensor = torch.tensor(y_train.values.astype(np.float32)).to(self.device)\n",
        "\n",
        "        dataset = TensorDataset(X_tensor, y_tensor)\n",
        "        dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in range(10):  # small epochs for demo\n",
        "            for xb, yb in dataloader:\n",
        "                preds = self.model(xb)\n",
        "                loss = binary_cross_entropy_with_logits(preds, yb)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "    def predict_v0(self, X: Optional[pd.DataFrame] = None) -> np.ndarray:\n",
        "        if X is None:\n",
        "            X = self.X_test\n",
        "\n",
        "        if self.model_name == \"mlp\":\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                X_tensor = torch.tensor(X.values, dtype=torch.float32).to(self.device)\n",
        "                logits = self.model(X_tensor)\n",
        "                probs = torch.sigmoid(logits)\n",
        "                return (probs > 0.5).cpu().numpy()\n",
        "        else:\n",
        "            return self.model.predict(X)\n",
        "\n",
        "    def fit(self):\n",
        "        # Align feature index with label index\n",
        "        X_aligned = self.X.loc[self.y.index.intersection(self.X.index)]\n",
        "        y_aligned = self.y.loc[X_aligned.index]\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_aligned, y_aligned,\n",
        "            test_size=self.test_size,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "\n",
        "        self.X_train, self.X_test = X_train, X_test\n",
        "        self.y_train, self.y_test = y_train, y_test\n",
        "\n",
        "        if self.model_name == \"mlp\":\n",
        "            input_dim = X_train.shape[1]\n",
        "            self.model = self._initialize_model(input_dim)\n",
        "            self._train_mlp(X_train, y_train)\n",
        "        else:\n",
        "            self.model = self._initialize_model()\n",
        "            # wrap the (single) fit call in a tiny tqdm bar\n",
        "            with tqdm(total=1, desc=f\"Fitting {self.model_name}\") as pbar:\n",
        "                self.model.fit(X_train, y_train)\n",
        "                pbar.update()\n",
        "\n",
        "    def _train_mlp(self, X_train: pd.DataFrame, y_train: pd.Series):\n",
        "        X_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(self.device)\n",
        "        y_tensor = torch.tensor(y_train.values.astype(np.float32)).to(self.device)\n",
        "\n",
        "        dataset = TensorDataset(X_tensor, y_tensor)\n",
        "        dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        self.model.train()\n",
        "\n",
        "        # outer loop: epochs\n",
        "        for epoch in tqdm(range(10), desc=\"Epochs\"):\n",
        "            # inner loop: batches\n",
        "            for xb, yb in tqdm(dataloader, desc=\"Batches\", leave=False):\n",
        "                preds = self.model(xb)\n",
        "                loss = binary_cross_entropy_with_logits(preds, yb)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "    def predict(self, X: Optional[pd.DataFrame] = None) -> np.ndarray:\n",
        "        if X is None:\n",
        "            X = self.X_test\n",
        "\n",
        "        if self.model_name == \"mlp\":\n",
        "            self.model.eval()\n",
        "            # turn X into batches so we can show a bar\n",
        "            dataset = TensorDataset(\n",
        "                torch.tensor(X.values, dtype=torch.float32)\n",
        "            )\n",
        "            dataloader = DataLoader(dataset, batch_size=64)\n",
        "            all_preds = []\n",
        "            with torch.no_grad():\n",
        "                for (xb,) in tqdm(dataloader, desc=\"Predicting Batches\"):\n",
        "                    xb = xb.to(self.device)\n",
        "                    logits = self.model(xb)\n",
        "                    probs = torch.sigmoid(logits)\n",
        "                    all_preds.append((probs > 0.5).cpu().numpy())\n",
        "            return np.concatenate(all_preds)\n",
        "        else:\n",
        "            # single-call predict; emulate a bar if you really need it\n",
        "            with tqdm(total=len(X), desc=f\"Predicting {self.model_name}\") as pbar:\n",
        "                preds = self.model.predict(X)\n",
        "                pbar.update(len(X))\n",
        "            return preds\n",
        "\n",
        "    def evaluate(self) -> Dict[str, Any]:\n",
        "        preds = self.predict()\n",
        "        if self.model_name == \"mlp\":\n",
        "            with torch.no_grad():\n",
        "                X_tensor = torch.tensor(self.X_test.values, dtype=torch.float32).to(self.device)\n",
        "                probs = torch.sigmoid(self.model(X_tensor)).cpu().numpy()\n",
        "        elif hasattr(self.model, \"predict_proba\"):\n",
        "            probs = self.model.predict_proba(self.X_test)[:, 1]\n",
        "        else:\n",
        "            probs = preds\n",
        "\n",
        "        report = classification_report(self.y_test, preds, output_dict=True)\n",
        "        auc = roc_auc_score(self.y_test, probs) if probs is not None else None\n",
        "\n",
        "        return {\n",
        "            \"classification_report\": report,\n",
        "            \"roc_auc\": auc,\n",
        "            \"model\": self.model_name,\n",
        "            \"probs\": probs\n",
        "        }\n",
        "\n",
        "class EnhancedFeatureGenerator(BaseModel):\n",
        "    etl: ETLPipeline\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    def extract_basic_statistics(self, before, after):\n",
        "        \"\"\"Extract basic statistical features from time series segments\"\"\"\n",
        "        features = {\n",
        "            \"mean_before\": before['value'].mean(),\n",
        "            \"std_before\": before['value'].std(),\n",
        "            \"skew_before\": before['value'].skew(),\n",
        "            \"kurtosis_before\": before['value'].kurtosis(),\n",
        "            \"mean_after\": after['value'].mean() if after is not None else np.nan,\n",
        "            \"std_after\": after['value'].std() if after is not None else np.nan,\n",
        "            \"skew_after\": after['value'].skew() if after is not None else np.nan,\n",
        "            \"kurtosis_after\": after['value'].kurtosis() if after is not None else np.nan\n",
        "        }\n",
        "\n",
        "        # Calculate delta features\n",
        "        if after is not None:\n",
        "            features.update({\n",
        "                \"length_pct\": len(after) / (len(before) + len(after)),\n",
        "                \"delta_mean\": (features[\"mean_after\"] / features[\"mean_before\"]) - 1\n",
        "                    if features[\"mean_before\"] != 0 else np.nan,\n",
        "                \"delta_std\": (features[\"std_after\"] / features[\"std_before\"]) - 1\n",
        "                    if features[\"std_before\"] != 0 else np.nan,\n",
        "                \"delta_skew\": (features[\"skew_after\"] / features[\"skew_before\"]) - 1\n",
        "                    if features[\"skew_before\"] != 0 else np.nan,\n",
        "                \"delta_kurtosis\": (features[\"kurtosis_after\"] / features[\"kurtosis_before\"]) - 1\n",
        "                    if features[\"kurtosis_before\"] != 0 else np.nan,\n",
        "                \"delta_mean_std\": (features[\"mean_before\"] / features[\"std_before\"]) -\n",
        "                    (features[\"mean_after\"] / features[\"std_after\"])\n",
        "                    if features[\"std_before\"] != 0 and features[\"std_after\"] != 0 else np.nan\n",
        "            })\n",
        "        else:\n",
        "            features.update({\n",
        "                \"length_pct\": np.nan,\n",
        "                \"delta_mean\": np.nan,\n",
        "                \"delta_std\": np.nan,\n",
        "                \"delta_skew\": np.nan,\n",
        "                \"delta_kurtosis\": np.nan,\n",
        "                \"delta_mean_std\": np.nan\n",
        "            })\n",
        "\n",
        "        return features\n",
        "\n",
        "    def extract_statistical_tests(self, before, after):\n",
        "        \"\"\"Extract statistical test features\"\"\"\n",
        "        tsc = TimeSeriesComparison()\n",
        "        x = before['value']\n",
        "        y = after['value'] if after is not None else x\n",
        "\n",
        "        features = {\n",
        "            'ks_pvalue': tsc.kolmogorov_smirnov_test(x, y).p_value,\n",
        "            'ad_pvalue': tsc.anderson_darling_test(x, y).p_value,\n",
        "            'mw_pvalue': tsc.mann_whitney_test(x, y).p_value,\n",
        "            'permutation_pvalue': tsc.permutation_test(x, y).p_value\n",
        "        }\n",
        "\n",
        "        # Add test statistics in addition to p-values\n",
        "        features.update({\n",
        "            'ks_statistic': tsc.kolmogorov_smirnov_test(x, y).statistic,\n",
        "            'ad_statistic': tsc.anderson_darling_test(x, y).statistic\n",
        "        })\n",
        "\n",
        "        return features\n",
        "\n",
        "    def extract_frequency_features(self, before, after):\n",
        "        \"\"\"Extract frequency domain features\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # Calculate power spectral density for both segments\n",
        "        def get_psd_features(series, prefix):\n",
        "            if len(series) < 4:\n",
        "                return {f\"{prefix}_psd_mean\": np.nan, f\"{prefix}_psd_std\": np.nan}\n",
        "\n",
        "            f, psd = signal.welch(series, fs=1.0, nperseg=min(len(series)//2, 256))\n",
        "            return {\n",
        "                f\"{prefix}_psd_mean\": np.mean(psd),\n",
        "                f\"{prefix}_psd_std\": np.std(psd),\n",
        "                f\"{prefix}_psd_max\": np.max(psd),\n",
        "                f\"{prefix}_psd_peak_freq\": f[np.argmax(psd)]\n",
        "            }\n",
        "\n",
        "        features.update(get_psd_features(before['value'], \"before\"))\n",
        "\n",
        "        if after is not None:\n",
        "            features.update(get_psd_features(after['value'], \"after\"))\n",
        "            # Calculate spectral distance features\n",
        "            features[\"psd_distance\"] = np.mean(np.abs(\n",
        "                signal.welch(before['value'], fs=1.0)[1] -\n",
        "                signal.welch(after['value'], fs=1.0)[1]\n",
        "            )) if min(len(before), len(after)) > 4 else np.nan\n",
        "\n",
        "        return features\n",
        "\n",
        "    def extract_autocorrelation_features(self, before, after):\n",
        "        \"\"\"Extract autocorrelation features\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # Calculate autocorrelation at different lags\n",
        "        def get_acf_features(series, prefix, max_lag=5): # 10\n",
        "            if len(series) <= max_lag:\n",
        "                return {f\"{prefix}_acf_lag{i}\": np.nan for i in range(1, max_lag+1)}\n",
        "\n",
        "            acf_values = sm.tsa.acf(series, nlags=max_lag, fft=True)\n",
        "            return {f\"{prefix}_acf_lag{i}\": acf_values[i] for i in range(1, min(max_lag+1, len(acf_values)))}\n",
        "\n",
        "        features.update(get_acf_features(before['value'], \"before\"))\n",
        "\n",
        "        if after is not None:\n",
        "            features.update(get_acf_features(after['value'], \"after\"))\n",
        "\n",
        "            # Calculate ACF distance features\n",
        "            before_acf = sm.tsa.acf(before['value'], nlags=min(10, len(before)-1), fft=True)\n",
        "            after_acf = sm.tsa.acf(after['value'], nlags=min(10, len(after)-1), fft=True)\n",
        "            min_len = min(len(before_acf), len(after_acf))\n",
        "            features[\"acf_distance\"] = np.mean(np.abs(before_acf[:min_len] - after_acf[:min_len]))\n",
        "\n",
        "        return features\n",
        "\n",
        "    def extract_entropy_features(self, before, after):\n",
        "        \"\"\"Extract entropy-based features\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # Sample entropy\n",
        "        def sample_entropy(series, m=2, r=0.2):\n",
        "            if len(series) < m+2:\n",
        "                return np.nan\n",
        "            # Normalize the series\n",
        "            series = (series - np.mean(series)) / np.std(series)\n",
        "            r = r * np.std(series)\n",
        "            return nolds.sampen(series, emb_dim=m, tolerance=r)\n",
        "\n",
        "        features[\"before_sample_entropy\"] = sample_entropy(before['value'])\n",
        "        features[\"after_sample_entropy\"] = sample_entropy(after['value']) if after is not None else np.nan\n",
        "\n",
        "        if after is not None:\n",
        "            features[\"delta_entropy\"] = features[\"after_sample_entropy\"] - features[\"before_sample_entropy\"]\n",
        "\n",
        "        return features\n",
        "\n",
        "    def extract_catch22_features(self, before, after):\n",
        "        \"\"\"Extract catch22 features as mentioned in the research paper\"\"\"\n",
        "        try:\n",
        "            import pycatch22\n",
        "            features = {}\n",
        "\n",
        "            # Extract all catch22 features for both segments\n",
        "            before_catch22 = pycatch22.catch22_all(before['value'])\n",
        "            features.update({f\"before_catch22_{name}\": value for name, value in zip(before_catch22['names'], before_catch22['values'])})\n",
        "\n",
        "            if after is not None:\n",
        "                after_catch22 = pycatch22.catch22_all(after['value'])\n",
        "                features.update({f\"after_catch22_{name}\": value for name, value in zip(after_catch22['names'], after_catch22['values'])})\n",
        "\n",
        "                # Calculate delta features\n",
        "                for i, name in enumerate(before_catch22['names']):\n",
        "                    before_val = before_catch22['values'][i]\n",
        "                    after_val = after_catch22['values'][i]\n",
        "                    if before_val != 0:\n",
        "                        features[f\"delta_catch22_{name}\"] = (after_val / before_val) - 1\n",
        "\n",
        "            return features\n",
        "        except ImportError:\n",
        "            # Fallback if pycatch22 is not available\n",
        "            return {}\n",
        "\n",
        "    def _split_series(self, ts, id_val=None):\n",
        "        \"\"\"Split time series into before/after segments based on period change\"\"\"\n",
        "        change_indices = ts.index[ts['period'].diff() == 1].tolist()\n",
        "\n",
        "        # Determine if this represents a true regime change\n",
        "        id_change = False\n",
        "        if id_val is not None and len(self.etl.y) > 0:\n",
        "            id_change = self.etl.y.get(id_val, False)\n",
        "\n",
        "        if len(self.etl.y)==0:\n",
        "            # use this case to split X dataframe if no target is provided\n",
        "            id_change = True\n",
        "\n",
        "        # if (not change_indices) or (id_val is not None and not id_change):\n",
        "        #     before = ts\n",
        "        #     after = ts\n",
        "        #     change_point = np.nan\n",
        "        # else:\n",
        "        #     change_point = change_indices[0]\n",
        "        #     before = ts.loc[:change_point]\n",
        "        #     after = ts.loc[change_point + 1:] if change_point + 1 in ts.index else None\n",
        "\n",
        "        # test\n",
        "        change_point = change_indices[0]\n",
        "        before = ts.loc[:change_point]\n",
        "        after = ts.loc[change_point + 1:] if change_point + 1 in ts.index else None\n",
        "\n",
        "        return before, after, change_point\n",
        "\n",
        "    def extract_features_for_id(self, id_val, is_training=False):\n",
        "        \"\"\"Unified feature extraction method for both training and testing\"\"\"\n",
        "        ts = self.etl.get_series_by_id(id_val)\n",
        "        before, after, change_point = self._split_series(ts, id_val if is_training else None)\n",
        "\n",
        "        features = {\"id\": id_val}\n",
        "\n",
        "        # Extract different types of features\n",
        "        features.update(self.extract_basic_statistics(before, after))\n",
        "        features.update(self.extract_autocorrelation_features(before, after))\n",
        "        features.update(self.extract_statistical_tests(before, after))\n",
        "        # features.update(self.extract_frequency_features(before, after))\n",
        "        # features.update(self.extract_entropy_features(before, after))\n",
        "        # features.update(self.extract_catch22_features(before, after))\n",
        "\n",
        "        return features\n",
        "\n",
        "    def generate_feature_dataframe(self):\n",
        "        \"\"\"Generate features for all IDs\"\"\"\n",
        "        all_ids = self.etl.get_ids()\n",
        "        feature_dicts = []\n",
        "\n",
        "        if len(self.etl.y)==0:\n",
        "            is_training = False # True\n",
        "        else:\n",
        "            is_training = len(self.etl.y) > 0\n",
        "\n",
        "        for id_val in all_ids:\n",
        "            try:\n",
        "                features = self.extract_features_for_id(id_val, is_training)\n",
        "                feature_dicts.append(features)\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping id {id_val} due to error: {str(e)}\")\n",
        "\n",
        "        df = pd.DataFrame(feature_dicts).set_index(\"id\")\n",
        "\n",
        "        # Handle NaN values\n",
        "        df = df.fillna(df.mean())\n",
        "\n",
        "        return df\n",
        "\n",
        "class EnhancedMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[64, 32, 16], dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        # Create hidden layers\n",
        "        for h_dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, h_dim),\n",
        "                nn.BatchNorm1d(h_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout_rate)\n",
        "            ])\n",
        "            prev_dim = h_dim\n",
        "\n",
        "        # Output layer\n",
        "        layers.append(nn.Linear(prev_dim, 1))\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(1)\n",
        "\n",
        "class EnhancedMLModelPipeline(BaseModel):\n",
        "    X: pd.DataFrame\n",
        "    y: pd.Series\n",
        "    X_train: Optional[pd.DataFrame] = None\n",
        "    y_train: Optional[pd.Series] = None\n",
        "    X_test: Optional[pd.DataFrame] = None\n",
        "    y_test: Optional[pd.Series] = None\n",
        "    model_name: str = Field(default=\"random_forest\")\n",
        "    model: Optional[Any] = None\n",
        "    cv_folds: int = 5\n",
        "    random_state: int = 42\n",
        "    device: str = Field(default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    return_probabilities: bool = Field(default=True)\n",
        "    scaler: StandardScaler = StandardScaler()\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    def _initialize_model(self, input_dim: int = None):\n",
        "        if self.model_name == \"logistic_regression\":\n",
        "            return LogisticRegression(max_iter=1000, C=0.1)\n",
        "        elif self.model_name == \"random_forest\":\n",
        "            return RandomForestClassifier(\n",
        "                n_estimators=200,\n",
        "                max_depth=10,\n",
        "                min_samples_split=5,\n",
        "                class_weight='balanced'\n",
        "            )\n",
        "        elif self.model_name == \"xgboost\":\n",
        "            return xgb.XGBClassifier(\n",
        "                use_label_encoder=False,\n",
        "                eval_metric='auc',\n",
        "                learning_rate=0.05,\n",
        "                n_estimators=300,\n",
        "                max_depth=5,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8\n",
        "            )\n",
        "        elif self.model_name == \"mlp\":\n",
        "            if input_dim is None:\n",
        "                raise ValueError(\"input_dim required for MLP model\")\n",
        "            return EnhancedMLP(input_dim).to(self.device)\n",
        "        elif self.model_name == \"ensemble\":\n",
        "            if input_dim is None:\n",
        "                raise ValueError(\"input_dim required for ensemble model\")\n",
        "            return VotingClassifier(\n",
        "                estimators=[\n",
        "                    ('rf', RandomForestClassifier(n_estimators=200)),\n",
        "                    ('xgb', xgb.XGBClassifier(use_label_encoder=False, eval_metric='auc')),\n",
        "                    ('lr', LogisticRegression(max_iter=1000))\n",
        "                ],\n",
        "                voting='soft'\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model: {self.model_name}\")\n",
        "\n",
        "    def preprocess_features(self, X):\n",
        "        \"\"\"Apply feature preprocessing\"\"\"\n",
        "        # Remove highly correlated features\n",
        "        correlation_matrix = X.corr().abs()\n",
        "        upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
        "        to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
        "        X_processed = X.drop(columns=to_drop, errors='ignore')\n",
        "\n",
        "        # Apply scaling\n",
        "        X_scaled = pd.DataFrame(\n",
        "            self.scaler.fit_transform(X_processed),\n",
        "            index=X_processed.index,\n",
        "            columns=X_processed.columns\n",
        "        )\n",
        "\n",
        "        return X_scaled, self.scaler, to_drop\n",
        "\n",
        "    def fit(self):\n",
        "        print(f\"Running model: {self.model_name}\")\n",
        "\n",
        "        # Align feature index with label index\n",
        "        X_aligned = self.X.loc[self.y.index.intersection(self.X.index)]\n",
        "        y_aligned = self.y.loc[X_aligned.index]\n",
        "\n",
        "        # Preprocess features\n",
        "        X_processed, self.scaler, dropped_features = self.preprocess_features(X_aligned)\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_processed, y_aligned, test_size=0.2, random_state=self.random_state\n",
        "        )\n",
        "\n",
        "        self.X_train, self.X_test = X_train, X_test\n",
        "        self.y_train, self.y_test = y_train, y_test\n",
        "\n",
        "        input_dim = X_train.shape[1]\n",
        "\n",
        "        if self.model_name == \"mlp\":\n",
        "            self.model = self._initialize_model(input_dim)\n",
        "            self._train_mlp(X_train, y_train)\n",
        "        else:\n",
        "            # Use cross-validation for hyperparameter tuning\n",
        "            if self.model_name == \"random_forest\":\n",
        "                param_grid = {\n",
        "                    'n_estimators': [100, 200, 300],\n",
        "                    'max_depth': [5, 10, None],\n",
        "                    'min_samples_split': [2, 5, 10]\n",
        "                }\n",
        "                base_model = RandomForestClassifier(class_weight='balanced')\n",
        "                grid_search = GridSearchCV(\n",
        "                    base_model, param_grid, cv=self.cv_folds, scoring='roc_auc'\n",
        "                )\n",
        "                grid_search.fit(X_train, y_train)\n",
        "                self.model = grid_search.best_estimator_\n",
        "            elif self.model_name == \"xgboost\":\n",
        "                param_grid = {\n",
        "                    'learning_rate': [0.01, 0.05, 0.1],\n",
        "                    'max_depth': [3, 5, 7],\n",
        "                    'n_estimators': [100, 200, 300]\n",
        "                }\n",
        "                base_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='auc')\n",
        "                grid_search = GridSearchCV(\n",
        "                    base_model, param_grid, cv=self.cv_folds, scoring='roc_auc'\n",
        "                )\n",
        "                grid_search.fit(X_train, y_train)\n",
        "                self.model = grid_search.best_estimator_\n",
        "            else:\n",
        "                self.model = self._initialize_model(input_dim)\n",
        "                self.model.fit(X_train, y_train)\n",
        "\n",
        "    def _train_mlp(self, X_train, y_train):\n",
        "        X_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(self.device)\n",
        "        y_tensor = torch.tensor(y_train.values.astype(np.float32)).to(self.device)\n",
        "\n",
        "        dataset = TensorDataset(X_tensor, y_tensor)\n",
        "        train_size = int(0.8 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        "        )\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        best_model_state = None\n",
        "        patience_counter = 0\n",
        "        max_patience = 10\n",
        "\n",
        "        for epoch in range(100):\n",
        "            # Training phase\n",
        "            self.model.train()\n",
        "            train_loss = 0\n",
        "            for xb, yb in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                preds = self.model(xb)\n",
        "                loss = criterion(preds, yb)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item() * xb.size(0)\n",
        "            train_loss /= len(train_loader.dataset)\n",
        "\n",
        "            # Validation phase\n",
        "            self.model.eval()\n",
        "            val_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for xb, yb in val_loader:\n",
        "                    preds = self.model(xb)\n",
        "                    loss = criterion(preds, yb)\n",
        "                    val_loss += loss.item() * xb.size(0)\n",
        "            val_loss /= len(val_loader.dataset)\n",
        "\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "            # Early stopping\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_model_state = copy.deepcopy(self.model.state_dict())\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if patience_counter >= max_patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        # Load best model\n",
        "        if best_model_state is not None:\n",
        "            self.model.load_state_dict(best_model_state)\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"Evaluate the model and return performance metrics\"\"\"\n",
        "        if self.model_name == \"mlp\":\n",
        "            self.model.eval()\n",
        "            X_test_tensor = torch.tensor(self.X_test.values, dtype=torch.float32).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                y_proba = torch.sigmoid(self.model(X_test_tensor)).cpu().numpy()\n",
        "                y_pred = (y_proba > 0.5).astype(int)\n",
        "        else:\n",
        "            y_proba = self.model.predict_proba(self.X_test)[:, 1]\n",
        "            y_pred = self.model.predict(self.X_test)\n",
        "\n",
        "        metrics_dict = {\n",
        "            \"accuracy\": accuracy_score(self.y_test, y_pred),\n",
        "            \"roc_auc\": roc_auc_score(self.y_test, y_proba),\n",
        "            \"precision\": precision_score(self.y_test, y_pred),\n",
        "            \"recall\": recall_score(self.y_test, y_pred),\n",
        "            \"f1\": f1_score(self.y_test, y_pred),\n",
        "            \"confusion_matrix\": confusion_matrix(self.y_test, y_pred).tolist()\n",
        "        }\n",
        "\n",
        "        return metrics_dict\n",
        "\n",
        "def list_files_in_directory(dir_path):\n",
        "  \"\"\"Lists all files in the specified directory.\n",
        "\n",
        "  Args:\n",
        "    dir_path: The path to the directory. If not specified, the current\n",
        "      working directory is used.\n",
        "\n",
        "  Returns:\n",
        "    A list of strings, where each string is the name of a file in the directory.\n",
        "    Returns an empty list if the directory does not exist or is empty.\n",
        "\n",
        "  Usage:\n",
        "    current_dir_files = list_files_in_directory(os.getcwd())\n",
        "    print(\"Files in current directory:\", current_dir_files)\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    files = os.listdir(dir_path)\n",
        "    return files\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Error: Directory '{dir_path}' not found.\")\n",
        "    return []\n",
        "\n",
        "def extract_values_by_id(df, n):\n",
        "    \"\"\"\n",
        "    Extract all rows for a given id number 'n' from a DataFrame\n",
        "    with a MultiIndex of ['id', 'time'].\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): The input DataFrame with a MultiIndex.\n",
        "        n (int): The id value to filter on.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Filtered DataFrame with rows corresponding to id 'n'.\n",
        "\n",
        "    Usage:\n",
        "     df = extract_values_by_id(X_train, id=3)\n",
        "     df['level'] = df[['value']].cumsum()\n",
        "     df[['level','period']].plot()\n",
        "     df[['value','period']].plot()\n",
        "     print(df.groupby('period').describe()['value'])\n",
        "     print(df.tail())\n",
        "     print(f\"y_train: {y_train.loc[id]}\")\n",
        "    \"\"\"\n",
        "    return df.loc[n]\n",
        "\n",
        "def split_df_by_id(df: pd.DataFrame, id_cut: int = None, n_random: int = None, seed: int = None):\n",
        "    \"\"\"\n",
        "    Splits a MultiIndex DataFrame by 'id'.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The input DataFrame with a MultiIndex ('id', 'time').\n",
        "    id_cut : int, optional\n",
        "        If provided, selects all ids <= id_cut for the first split.\n",
        "    n_random : int, optional\n",
        "        If provided, randomly selects this many unique ids for the first split.\n",
        "    seed : int, optional\n",
        "        Random seed for reproducibility when using n_random.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    df1, df2 : pd.DataFrame\n",
        "        Tuple of DataFrames:\n",
        "          - df1 contains rows for the selected ids.\n",
        "          - df2 contains rows for all other ids.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    Exactly one of `id_cut` or `n_random` must be provided.\n",
        "    \"\"\"\n",
        "    # Extract unique ids\n",
        "    ids = df.index.get_level_values('id').unique()\n",
        "\n",
        "    # Determine selected ids based on threshold or random sampling\n",
        "    if id_cut is not None and n_random is None:\n",
        "        selected_ids = ids[ids <= id_cut]\n",
        "    elif n_random is not None and id_cut is None:\n",
        "        if n_random > len(ids):\n",
        "            raise ValueError(f\"n_random={n_random} exceeds the number of unique ids={len(ids)}\")\n",
        "        rng = np.random.default_rng(seed)\n",
        "        selected_ids = rng.choice(ids, size=n_random, replace=False)\n",
        "    else:\n",
        "        raise ValueError(\"Provide exactly one of `id_cut` or `n_random`\")\n",
        "\n",
        "    # Split the DataFrame\n",
        "    df1 = df.loc[selected_ids]\n",
        "    df2 = df.loc[~df.index.get_level_values('id').isin(selected_ids)]\n",
        "\n",
        "    return df1, df2\n",
        "\n",
        "# function definitions for training and inference\n",
        "def train(\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        "    model_directory_path: str,\n",
        "    model_name = \"xgboost\" # # \"logistic_regression\", \"random_forest\", \"xgboost\", \"lightgbm\", \"mlp\"\n",
        "):\n",
        "    # For our baseline t-test approach, we don't need to train a model\n",
        "    # This is essentially an unsupervised approach calculated at inference time\n",
        "    if not isinstance(y_train, pd.Series):\n",
        "        y_train = y_train.iloc[:, 0]\n",
        "\n",
        "    \"\"\"\n",
        "    # ETL pipeline\n",
        "    \"\"\"\n",
        "    etl = ETLPipeline(X=X_train, y=y_train)\n",
        "\n",
        "    \"\"\"\n",
        "    # Feature generator\n",
        "    \"\"\"\n",
        "    generator = EnhancedFeatureGenerator(etl=etl)\n",
        "    X_features = generator.generate_feature_dataframe()\n",
        "\n",
        "    \"\"\"\n",
        "    # ML model pipeline\n",
        "    \"\"\"\n",
        "    valid_index = X_features.loc[~X_features.isna().any(axis=1)].index\n",
        "    X = X_features.loc[valid_index]\n",
        "    y = etl.y[valid_index]\n",
        "\n",
        "    model = MLModelPipeline(X=X, y=y, model_name=model_name)\n",
        "    model.fit()\n",
        "    # model = None\n",
        "\n",
        "    joblib.dump(model, os.path.join(model_directory_path, 'model.joblib'))\n",
        "\n",
        "def infer(\n",
        "    X_test: typing.Iterable[pd.DataFrame],\n",
        "    model_directory_path: str):\n",
        "    \"\"\"\n",
        "    Note: This baseline approach uses a t-test to compare the distributions\n",
        "     before and after the boundary point. A smaller p-value (larger negative number)\n",
        "     suggests stronger evidence that the distributions are different,\n",
        "     indicating a potential structural break.\n",
        "    \"\"\"\n",
        "    model = joblib.load(os.path.join(model_directory_path, 'model.joblib'))\n",
        "\n",
        "    yield  # Mark as ready\n",
        "\n",
        "    # X_test can only be iterated once.\n",
        "    # Before getting the next dataset, you must predict the current one.\n",
        "    for dataset in X_test:\n",
        "        etl = ETLPipeline(X=X_test)\n",
        "\n",
        "        generator = EnhancedFeatureGenerator(etl=etl)\n",
        "        X_features = generator.generate_feature_dataframe()\n",
        "\n",
        "        valid_index = X_features.loc[~X_features.isna().any(axis=1)].index\n",
        "        X = X_features.loc[valid_index]\n",
        "        prediction = model.predict(X)\n",
        "\n",
        "        yield prediction  # Send the prediction for the current dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "8eL0rCKyMEv5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjD_WSAS-0fR",
        "outputId": "42fe3d75-1afb-40db-c9d0-912e2a98d1c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded inline runner with module: <module '__main__'>\n",
            "\n",
            "cli version: 6.5.0\n",
            "available ram: 12.67 gb\n",
            "available cpu: 2 core\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "import crunch\n",
        "\n",
        "# Load the Crunch Toolings\n",
        "crunch = crunch.load_notebook()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiKJODFx-0fR"
      },
      "source": [
        "## Understanding the Data\n",
        "\n",
        "The dataset consists of univariate time series, each containing ~2,000-5,000 values with a designated boundary point. For each time series, you need to determine whether a structural break occurred at this boundary point.\n",
        "\n",
        "The data was downloaded when you setup your local environment and is now available in the `data/` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKHXgvjN-0fS",
        "outputId": "46e688de-d15f-4bab-a0c1-25e4621330a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/X_train.parquet: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
            "data/X_train.parquet: already exists, file length match\n",
            "data/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
            "data/X_test.reduced.parquet: already exists, file length match\n",
            "data/y_train.parquet: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
            "data/y_train.parquet: already exists, file length match\n",
            "data/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
            "data/y_test.reduced.parquet: already exists, file length match\n"
          ]
        }
      ],
      "source": [
        "# Load the data simply\n",
        "X_train, y_train, X_test = crunch.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T_JmgMq-0fS"
      },
      "source": [
        "### Understanding `X_train`\n",
        "\n",
        "The training data is structured as a pandas DataFrame with a MultiIndex:\n",
        "\n",
        "**Index Levels:**\n",
        "- `id`: Identifies the unique time series\n",
        "- `time`: The timestep within each time series\n",
        "\n",
        "**Columns:**\n",
        "- `value`: The actual time series value at each timestep\n",
        "- `period`: A binary indicator where `0` represents the **period before** the boundary point, and `1` represents the **period after** the boundary point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "0oRCTnOb-0fS",
        "outputId": "fd7dba72-d53d-4e58-b4b5-c4174aab47c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               value  period\n",
              "id    time                  \n",
              "0     0    -0.005564       0\n",
              "      1     0.003705       0\n",
              "      2     0.013164       0\n",
              "      3     0.007151       0\n",
              "      4    -0.009979       0\n",
              "...              ...     ...\n",
              "10000 2134  0.001137       1\n",
              "      2135  0.003526       1\n",
              "      2136  0.000687       1\n",
              "      2137  0.001640       1\n",
              "      2138  0.001074       1\n",
              "\n",
              "[23715734 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e7ade699-e6d4-40d0-80d6-f231e669bc39\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>value</th>\n",
              "      <th>period</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th>time</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
              "      <th>0</th>\n",
              "      <td>-0.005564</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.003705</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.013164</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.007151</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.009979</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">10000</th>\n",
              "      <th>2134</th>\n",
              "      <td>0.001137</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2135</th>\n",
              "      <td>0.003526</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2136</th>\n",
              "      <td>0.000687</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2137</th>\n",
              "      <td>0.001640</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2138</th>\n",
              "      <td>0.001074</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>23715734 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e7ade699-e6d4-40d0-80d6-f231e669bc39')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e7ade699-e6d4-40d0-80d6-f231e669bc39 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e7ade699-e6d4-40d0-80d6-f231e669bc39');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5c8be626-9e83-4650-9889-936c7c79c86a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5c8be626-9e83-4650-9889-936c7c79c86a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5c8be626-9e83-4650-9889-936c7c79c86a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_337ee3ee-46df-4c6d-99aa-69333404fb71\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('X_train')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_337ee3ee-46df-4c6d-99aa-69333404fb71 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('X_train');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X_train"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WP39dgx-0fS"
      },
      "source": [
        "### Understanding `y_train`\n",
        "\n",
        "This is a simple `pandas.Series` that tells if a dataset id has a structural breakpoint or not.\n",
        "\n",
        "**Index:**\n",
        "- `id`: the ID of the dataset\n",
        "\n",
        "**Value:**\n",
        "- `structural_breakpoint`: Boolean indicating whether a structural break occurred (`True`) or not (`False`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "dPsQPdIj-0fT",
        "outputId": "45c94a26-8165-4fb5-e9e7-a3a714f8bedf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "0        False\n",
              "1        False\n",
              "2         True\n",
              "3        False\n",
              "4        False\n",
              "         ...  \n",
              "9996     False\n",
              "9997     False\n",
              "9998     False\n",
              "9999     False\n",
              "10000     True\n",
              "Name: structural_breakpoint, Length: 10001, dtype: bool"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>structural_breakpoint</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10000</th>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10001 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> bool</label>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oSS08Ks-0fT"
      },
      "source": [
        "### Understanding `X_test`\n",
        "\n",
        "The test data is provided as a **`list` of `pandas.DataFrame`s** with the same format as [`X_train`](#understanding-X_test).\n",
        "\n",
        "It is structured as a list to encourage processing records one by one, which will be mandatory in the `infer()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "8ErbKAs--0fT",
        "outputId": "0fd97aa0-270a-44ef-87d9-d485369b86ab"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Number of datasets: \u001b[1;36m101\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Number of datasets: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">101</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(\"Number of datasets:\", len(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "M_dTYXms-0fT",
        "outputId": "4a358a08-f15c-4ff6-873d-aeaaa2629991"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               value  period\n",
              "id    time                  \n",
              "10001 0     0.010753       0\n",
              "      1    -0.031915       0\n",
              "      2    -0.010989       0\n",
              "      3    -0.011111       0\n",
              "      4     0.011236       0\n",
              "...              ...     ...\n",
              "      2774 -0.013937       1\n",
              "      2775 -0.015649       1\n",
              "      2776 -0.009744       1\n",
              "      2777  0.025375       1\n",
              "      2778 -0.001515       1\n",
              "\n",
              "[2779 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5e83485a-3dc9-4ed5-a618-92b4bc9d3238\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>value</th>\n",
              "      <th>period</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th>time</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"11\" valign=\"top\">10001</th>\n",
              "      <th>0</th>\n",
              "      <td>0.010753</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.031915</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.010989</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.011111</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.011236</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2774</th>\n",
              "      <td>-0.013937</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2775</th>\n",
              "      <td>-0.015649</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2776</th>\n",
              "      <td>-0.009744</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2777</th>\n",
              "      <td>0.025375</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2778</th>\n",
              "      <td>-0.001515</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2779 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5e83485a-3dc9-4ed5-a618-92b4bc9d3238')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5e83485a-3dc9-4ed5-a618-92b4bc9d3238 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5e83485a-3dc9-4ed5-a618-92b4bc9d3238');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8eb96bd7-3033-4e16-b438-ecd89b9bd7e1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8eb96bd7-3033-4e16-b438-ecd89b9bd7e1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8eb96bd7-3033-4e16-b438-ecd89b9bd7e1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"X_test[0]\",\n  \"rows\": 2779,\n  \"fields\": [\n    {\n      \"column\": \"value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02972494368757725,\n        \"min\": -0.322581,\n        \"max\": 0.176991,\n        \"num_unique_values\": 1939,\n        \"samples\": [\n          -0.00052267,\n          -0.027999999999999997,\n          -0.0256409\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"period\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "X_test[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgulFOGX-0fT"
      },
      "source": [
        "## Strategy Implementation\n",
        "\n",
        "There are multiple approaches you can take to detect structural breaks:\n",
        "\n",
        "1. **Statistical Tests**: Compare distributions before and after the boundary point;\n",
        "2. **Feature Engineering**: Extract features from both segments for comparison;\n",
        "3. **Time Series Modeling**: Detect deviations from expected patterns;\n",
        "4. **Machine Learning**: Train models to recognize break patterns from labeled examples.\n",
        "\n",
        "The baseline implementation below uses a simple statistical approach: a t-test to compare the distributions before and after the boundary point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLfYIXlz-0fT"
      },
      "source": [
        "### The `train()` Function\n",
        "\n",
        "In this function, you build and train your model for making inferences on the test data. Your model must be stored in the `model_directory_path`.\n",
        "\n",
        "The baseline implementation below doesn't require a pre-trained model, as it uses a statistical test that will be computed at inference time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-18T10:04:00.459399Z",
          "start_time": "2024-11-18T10:04:00.455716Z"
        },
        "id": "xQwWDC6M-0fT"
      },
      "outputs": [],
      "source": [
        "# def train(\n",
        "#     X_train: pd.DataFrame,\n",
        "#     y_train: pd.Series,\n",
        "#     model_directory_path: str,\n",
        "# ):\n",
        "#     # For our baseline t-test approach, we don't need to train a model\n",
        "#     # This is essentially an unsupervised approach calculated at inference time\n",
        "#     model = None\n",
        "\n",
        "#     # You could enhance this by training an actual model, for example:\n",
        "#     # 1. Extract features from before/after segments of each time series\n",
        "#     # 2. Train a classifier using these features and y_train labels\n",
        "#     # 3. Save the trained model\n",
        "\n",
        "#     joblib.dump(model, os.path.join(model_directory_path, 'model.joblib'))\n",
        "\n",
        "def train(\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        "    model_directory_path: str,\n",
        "    model_name = \"xgboost\" # # \"logistic_regression\", \"random_forest\", \"xgboost\", \"lightgbm\", \"mlp\"\n",
        "):\n",
        "    # For our baseline t-test approach, we don't need to train a model\n",
        "    # This is essentially an unsupervised approach calculated at inference time\n",
        "    if not isinstance(y_train, pd.Series):\n",
        "        y_train = y_train.iloc[:, 0]\n",
        "\n",
        "    \"\"\"\n",
        "    # ETL pipeline\n",
        "    \"\"\"\n",
        "    etl = ETLPipeline(X=X_train, y=y_train)\n",
        "\n",
        "    \"\"\"\n",
        "    # Feature generator\n",
        "    \"\"\"\n",
        "    generator = EnhancedFeatureGenerator(etl=etl)\n",
        "    X_features = generator.generate_feature_dataframe()\n",
        "\n",
        "    \"\"\"\n",
        "    # ML model pipeline\n",
        "    \"\"\"\n",
        "    valid_index = X_features.loc[~X_features.isna().any(axis=1)].index\n",
        "    X = X_features.loc[valid_index]\n",
        "    y = etl.y[valid_index]\n",
        "\n",
        "    model = MLModelPipeline(X=X, y=y, model_name=model_name)\n",
        "    model.fit()\n",
        "    # model = None\n",
        "\n",
        "    joblib.dump(model, os.path.join(model_directory_path, 'model.joblib'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n-jboJH-0fU"
      },
      "source": [
        "### The `infer()` Function\n",
        "\n",
        "In the inference function, your trained model (if any) is loaded and used to make predictions on test data.\n",
        "\n",
        "**Important workflow:**\n",
        "1. Load your model;\n",
        "2. Use the `yield` statement to signal readiness to the runner;\n",
        "3. Process each dataset one by one within the for loop;\n",
        "4. For each dataset, use `yield prediction` to return your prediction.\n",
        "\n",
        "**Note:** The datasets can only be iterated once!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-18T10:03:59.120294Z",
          "start_time": "2024-11-18T10:03:59.114830Z"
        },
        "id": "r1b7hRkl-0fU"
      },
      "outputs": [],
      "source": [
        "# def infer(\n",
        "#     X_test: typing.Iterable[pd.DataFrame],\n",
        "#     model_directory_path: str,\n",
        "# ):\n",
        "#     model = joblib.load(os.path.join(model_directory_path, 'model.joblib'))\n",
        "\n",
        "#     yield  # Mark as ready\n",
        "\n",
        "#     # X_test can only be iterated once.\n",
        "#     # Before getting the next dataset, you must predict the current one.\n",
        "#     for dataset in X_test:\n",
        "#         # Baseline approach: Compute t-test between values before and after boundary point\n",
        "#         # The negative p-value is used as our score - smaller p-values (larger negative numbers)\n",
        "#         # indicate more evidence against the null hypothesis that distributions are the same,\n",
        "#         # suggesting a structural break\n",
        "#         def t_test(u: pd.DataFrame):\n",
        "#             return -scipy.stats.ttest_ind(\n",
        "#                 u[\"value\"][u[\"period\"] == 0],  # Values before boundary point\n",
        "#                 u[\"value\"][u[\"period\"] == 1],  # Values after boundary point\n",
        "#             ).pvalue\n",
        "\n",
        "#         prediction = t_test(dataset)\n",
        "#         yield prediction  # Send the prediction for the current dataset\n",
        "\n",
        "#         # Note: This baseline approach uses a t-test to compare the distributions\n",
        "#         # before and after the boundary point. A smaller p-value (larger negative number)\n",
        "#         # suggests stronger evidence that the distributions are different,\n",
        "#         # indicating a potential structural break.\n",
        "\n",
        "def infer(\n",
        "    X_test: typing.Iterable[pd.DataFrame],\n",
        "    model_directory_path: str):\n",
        "    \"\"\"\n",
        "    Note: This baseline approach uses a t-test to compare the distributions\n",
        "     before and after the boundary point. A smaller p-value (larger negative number)\n",
        "     suggests stronger evidence that the distributions are different,\n",
        "     indicating a potential structural break.\n",
        "    \"\"\"\n",
        "    model = joblib.load(os.path.join(model_directory_path, 'model.joblib'))\n",
        "\n",
        "    yield  # Mark as ready\n",
        "\n",
        "    # X_test can only be iterated once.\n",
        "    # Before getting the next dataset, you must predict the current one.\n",
        "    for dataset in X_test:\n",
        "        etl = ETLPipeline(X=dataset)\n",
        "\n",
        "        generator = EnhancedFeatureGenerator(etl=etl)\n",
        "        X_features = generator.generate_feature_dataframe()\n",
        "\n",
        "        valid_index = X_features.loc[~X_features.isna().any(axis=1)].index\n",
        "        X = X_features.loc[valid_index]\n",
        "        prediction = model.predict(X)\n",
        "\n",
        "        yield prediction  # Send the prediction for the current dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W0Kl9CA-0fU"
      },
      "source": [
        "## Local testing\n",
        "\n",
        "To make sure your `train()` and `infer()` function are working properly, you can call the `crunch.test()` function that will reproduce the cloud environment locally. <br />\n",
        "Even if it is not perfect, it should give you a quick idea if your model is working properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 996,
          "referenced_widgets": [
            "94743f325aba4d7796853e7acd0a4580",
            "f9958c3ddc3d43dfa6e215d079159b6d",
            "c9f52cfaa12a43eb81edced630b1b4c6",
            "093f4b3d21ef4b0fa7066faa2ce76eab",
            "965c71817af045719aa3199c2b0e8a76",
            "8a56db02250d48bc99c9f88752c02b5e",
            "163b59ca19864d8b929e1c46ca9fbcbf",
            "56c24f83864c4ed08769faead5c39213",
            "b0b6f721e72c48bc907b243db77f9a20",
            "22c564f946b64092acc9b0ef2a3d05e0",
            "69af15e0a70a497c8c27e5c3ed377a77"
          ]
        },
        "id": "tDZeP-4--0fU",
        "outputId": "5fe1f215-12fa-4408-831b-ba27f18a3010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "19:25:14 no forbidden library found\n",
            "19:25:14 \n",
            "19:25:14 started\n",
            "19:25:14 running local test\n",
            "19:25:14 internet access isn't restricted, no check will be done\n",
            "19:25:14 \n",
            "19:25:16 starting unstructured loop...\n",
            "19:25:16 executing - command=train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/X_train.parquet: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
            "data/X_train.parquet: already exists, file length match\n",
            "data/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
            "data/X_test.reduced.parquet: already exists, file length match\n",
            "data/y_train.parquet: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
            "data/y_train.parquet: already exists, file length match\n",
            "data/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
            "data/y_test.reduced.parquet: already exists, file length match\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:train: skip param with default value: model_name=xgboost\n",
            "<ipython-input-6-e741d9cda7ba>:173: UserWarning: p-value capped: true value larger than 0.25. Consider specifying `method` (e.g. `method=stats.PermutationMethod()`.)\n",
            "  result = stats.anderson_ksamp(samples)\n",
            "<ipython-input-6-e741d9cda7ba>:173: UserWarning: p-value floored: true value smaller than 0.001. Consider specifying `method` (e.g. `method=stats.PermutationMethod()`.)\n",
            "  result = stats.anderson_ksamp(samples)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fitting xgboost:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94743f325aba4d7796853e7acd0a4580"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [19:39:01] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "19:39:02 executing - command=infer\n",
            "19:39:02 duration - time=00:13:47\n",
            "19:39:02 memory - before=\"1.24 GB\" after=\"1.31 GB\" consumed=\"64.55 MB\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "1 validation error for ETLPipeline\nX\n  Input should be an instance of DataFrame [type=is_instance_of, input_value=<generator object Generat...inner at 0x7aa798117e60>, input_type=generator]\n    For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-61a19880640e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m crunch.test(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Uncomment to disable the train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# force_first_train=False,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Uncomment to disable the determinism check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crunch/inline.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, force_first_train, train_frequency, raise_abort, round_number, no_checks, no_determinism_check, read_kwargs, write_kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             return tester.run(\n\u001b[0m\u001b[1;32m    180\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_runner_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crunch/tester.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(user_module, runner_module, model_directory_path, force_first_train, train_frequency, round_number, competition, has_gpu, checks, no_determinism_check, read_kwargs, write_kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m     )\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crunch/runner/local.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             self.log(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crunch/runner/runner.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompetition_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompetitionFormat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSTRUCTURED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"starting unstructured loop...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_unstructured\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crunch/runner/local.py\u001b[0m in \u001b[0;36mstart_unstructured\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLocalRunnerContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         prediction = self.runner_module.run(\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_directory_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crunch/unstructured/module/runner.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, context, data_directory_path, model_directory_path)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mmodel_directory_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     ) -> typing.Any:\n\u001b[0;32m---> 34\u001b[0;31m         return execute.call_function(\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_run_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             {\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crunch/unstructured/execute.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(function, kwargs, print)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n\\ncalling {function}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         return utils.smart_call(\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crunch/utils.py\u001b[0m in \u001b[0;36msmart_call\u001b[0;34m(function, default_values, specific_values, log, logger)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mhttps://github.com/crunchdao/competitions/raw/refs/heads/master/competitions/structural-break/scoring/runner.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(context)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crunch/runner/local.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, command, parameters, return_prediction)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m         result = utils.smart_call(\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mparameters\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crunch/utils.py\u001b[0m in \u001b[0;36msmart_call\u001b[0;34m(function, default_values, specific_values, log, logger)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mhttps://github.com/crunchdao/competitions/raw/refs/heads/master/competitions/structural-break/scoring/runner.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(determinism_check)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/crunch/container.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self, expected_size)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentinel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0msentinel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-b0468f500120>\u001b[0m in \u001b[0;36minfer\u001b[0;34m(X_test, model_directory_path)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Before getting the next dataset, you must predict the current one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0metl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mETLPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnhancedFeatureGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mvalidated_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidated_self\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ETLPipeline\nX\n  Input should be an instance of DataFrame [type=is_instance_of, input_value=<generator object Generat...inner at 0x7aa798117e60>, input_type=generator]\n    For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of"
          ]
        }
      ],
      "source": [
        "crunch.test(\n",
        "    # Uncomment to disable the train\n",
        "    # force_first_train=False,\n",
        "\n",
        "    # Uncomment to disable the determinism check\n",
        "    # no_determinism_check=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV_5CKs--0fU"
      },
      "source": [
        "## Results\n",
        "\n",
        "Once the local tester is done, you can preview the result stored in `data/prediction.parquet`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ly5q68sA-0fU"
      },
      "outputs": [],
      "source": [
        "prediction = pd.read_parquet(\"data/prediction.parquet\")\n",
        "prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oP-NLGh-0fU"
      },
      "source": [
        "### Local scoring\n",
        "\n",
        "You can call the function that the system uses to estimate your score locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyCrjpzv-0fU"
      },
      "outputs": [],
      "source": [
        "# Load the targets\n",
        "target = pd.read_parquet(\"data/y_test.reduced.parquet\")[\"structural_breakpoint\"]\n",
        "\n",
        "# Call the scoring function\n",
        "sklearn.metrics.roc_auc_score(\n",
        "    target,\n",
        "    prediction,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AE1i3pR-0fV"
      },
      "source": [
        "# Submit your Notebook\n",
        "\n",
        "To submit your work, you must:\n",
        "1. Download your Notebook from Colab\n",
        "2. Upload it to the platform\n",
        "3. Create a run to validate it\n",
        "\n",
        "### >> https://hub.crunchdao.com/competitions/structural-break/submit/notebook\n",
        "\n",
        "![Download and Submit Notebook](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/documentation/animations/download-and-submit-notebook.gif)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "94743f325aba4d7796853e7acd0a4580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9958c3ddc3d43dfa6e215d079159b6d",
              "IPY_MODEL_c9f52cfaa12a43eb81edced630b1b4c6",
              "IPY_MODEL_093f4b3d21ef4b0fa7066faa2ce76eab"
            ],
            "layout": "IPY_MODEL_965c71817af045719aa3199c2b0e8a76"
          }
        },
        "f9958c3ddc3d43dfa6e215d079159b6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a56db02250d48bc99c9f88752c02b5e",
            "placeholder": "​",
            "style": "IPY_MODEL_163b59ca19864d8b929e1c46ca9fbcbf",
            "value": "Fitting xgboost: 100%"
          }
        },
        "c9f52cfaa12a43eb81edced630b1b4c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56c24f83864c4ed08769faead5c39213",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0b6f721e72c48bc907b243db77f9a20",
            "value": 1
          }
        },
        "093f4b3d21ef4b0fa7066faa2ce76eab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22c564f946b64092acc9b0ef2a3d05e0",
            "placeholder": "​",
            "style": "IPY_MODEL_69af15e0a70a497c8c27e5c3ed377a77",
            "value": " 1/1 [00:01&lt;00:00,  1.27s/it]"
          }
        },
        "965c71817af045719aa3199c2b0e8a76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a56db02250d48bc99c9f88752c02b5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "163b59ca19864d8b929e1c46ca9fbcbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56c24f83864c4ed08769faead5c39213": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0b6f721e72c48bc907b243db77f9a20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22c564f946b64092acc9b0ef2a3d05e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69af15e0a70a497c8c27e5c3ed377a77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}